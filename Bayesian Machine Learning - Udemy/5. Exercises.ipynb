{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "### Conjugate Priors\n",
    "\n",
    "> it is the centerpiece of the Bayesian method\n",
    "\n",
    "- In this course, since we've been thinking about ecommerce applications, we've been looking at CTR and conversion rates. \n",
    "- This means likelihood is Bernoulli and the prior is the Beta distribution\n",
    "- Are there any other conjugate priors?\n",
    "    - Closed=form solutions are the __exception__ not the rule\n",
    "\n",
    "### 1. Categorical distributions\n",
    "\n",
    "- Just an extension of the Bernoulli, but with more possible outcomes (e.g. die roll)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Likelihood**\n",
    "\n",
    "l(true) = 1, l(false) = 0\n",
    "\n",
    "![die_roll_l](img/die_roll_likelihood.png)\n",
    "\n",
    "**Prior**\n",
    "\n",
    "The Categorical likelihood is conjugate with the [**Dirichlet Prior**](https://en.wikipedia.org/wiki/Categorical_distribution#Bayesian_inference_using_conjugate_prior)\n",
    "\n",
    "> In Bayesian statistics, the Dirichlet distribution is the conjugate prior distribution of the categorical distribution (and also the multinomial distribution). This means that in a model consisting of a data point having a categorical distribution with unknown parameter vector p, and (in standard Bayesian style) we choose to treat this parameter as a random variable and give it a prior distribution defined using a Dirichlet distribution, then the posterior distribution of the parameter, after incorporating the knowledge gained from the observed data, is also a Dirichlet. (from Wikipedia)\n",
    "\n",
    "**Dirichlet Prior**\n",
    "\n",
    "This looks like this: \n",
    "\n",
    "![dirichlet](img/dirichlet.png)\n",
    "\n",
    "**Dirichlet Posterior**\n",
    "\n",
    "The Dirichlet Prior + Categorical Likelihood will give a posterior that is also Dirichlet distributed. And it will look like this: \n",
    "\n",
    "![dirichlet_posterior](img/dirichlet_posterior.png)\n",
    "\n",
    "This means that the alpha for every k (k being the specific category, e.g. 1,2,6, orange,etc) will be the previous alphas + the sum of the new ks, or the times that k appeared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gaussian likelihood \n",
    "- useful when we're measuring something where the likelihood is a real number\n",
    "- so we use the Gaussian likelihood\n",
    "- The Gaussian has 2 parameters:\n",
    "    - mean\n",
    "    - variance\n",
    "- Here, we will only place a prior on the mean (and assume the variance is fixed) --> we can do more later!\n",
    "\n",
    "\n",
    "**How**\n",
    "- we will use `precision`, which is the inverse of variance:\n",
    "\n",
    "`lambda^(-1) = sigma^2`\n",
    "\n",
    "**Gaussian conjugate priors**\n",
    "\n",
    "Both likelihood and prior will be modelled as follows:\n",
    "\n",
    "![Gaussian_prior](img/gaussian_conjugate_prior.png)\n",
    "\n",
    "**Solving for the posterior**\n",
    "\n",
    "Once we solve (many steps involved), we get to this: \n",
    "\n",
    "![gaussian_posterior](img/gaussian_posterior_solution.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
