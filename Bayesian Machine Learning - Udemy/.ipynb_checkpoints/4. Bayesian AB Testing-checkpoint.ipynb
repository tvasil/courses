{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore vs Exploit dilemma\n",
    "\n",
    "- **Example**: you \"know\" a drug is working, but Frequentist statistics say you shouldn't stop until the originally determined sample size is reached. That is because we are increasing the chance of finding false positives (nature of frequentist statistics)\n",
    "     - as we saw before, the p-value can go below and over the threshold over time (unstable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandit problem\n",
    "\n",
    "- You're at a casino playing slots (pulling the \"arm)\n",
    "- Machines have different payout probabilities\n",
    "     - 1 pays 30% of the time, 1 pays 20% of the time and 1 pays 10% of the time\n",
    "- in frequentist statistics you would do an A/B test for a specific # N of trials and then calculate the p-value. \n",
    "    - But this has a cost associated with it!\n",
    "\n",
    "What would you do in real life?\n",
    "- you would probably adapt based on the data you collect at every \"arm pull\"\n",
    "    - 3 plays probably not enough to attain significance, but you still feel compelled to believe that the first arm is better. \n",
    "    \n",
    "**Related: Reinforcement Learning**\n",
    "- teach a machine to play a game\n",
    "    - faces the same problem\n",
    "- it models rewards it gets based on the previous action\n",
    "- but the process could be stochastic (~random!), so the reward estimates will be approxmate\n",
    "- **Early on in the learning**: few actions have been taken, so we're unsure about most rewards\n",
    "    - We can't \"choose the action that leads to best reward\", because our current, early knowledge about the reward is minimal\n",
    "    - Only after collecting a lot of data will the estimate be accurate\n",
    "    \n",
    "    \n",
    "This is the **Explpore/Exploit dilemma**\n",
    "\n",
    "- if I get 3/3 from bandit 1, 0.3 from bandit 2, should I:\n",
    "    - exploit bandit 1 more? or\n",
    "    - explore at random to gather more data?\n",
    "- we'll look at several solutions, not all Bayesian (but all adaptive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy algorithm\n",
    "\n",
    "- if you have an experiment for 2 ads, the idea is that you will adapt which advertisement you show more often based on which one is performing better\n",
    "- You can **still** do frequentist A/B test after collecting the data, since the contingency table for chi-square test doesn't require both sampels ot be of the same size\n",
    "- The thing that changes here is the **machine that serves the ads**\n",
    "    - it will adapt to the performance of each ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works**\n",
    "\n",
    "- Choose a small number `epsilon` between 0 and 1: that is the probability of exploration\n",
    "- Create a while loop that generates random number `r`\n",
    "    - if `r < epsilon`, then explore\n",
    "    - if `r > epsilon`, then exploit\n",
    "    \n",
    "**Problems**\n",
    "- it will keep doing the same thing forever\n",
    "- even if A is statistically significantly bettwen than B, it will still sometimes show B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another algorithm: UCB1\n",
    "\n",
    "#### Confidence intervals\n",
    "- From previous examples, we define the upper and lower limit to represent where we belive the true Conversion Rate (CTR) is\n",
    "- we'll now look at a similar idea, but using a \"tighter\" bound, **Chernoff-Hoeffding bound**\n",
    "\n",
    "![chbound](img/ch_bound.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each arm j, we \"choose\" the epsilon: \n",
    "\n",
    "![epsilon](img/epsilon_ucb.png)\n",
    "\n",
    "where\n",
    "- N = total games played so far\n",
    "- Nj = total times played arm j (so far)\n",
    "\n",
    "\n",
    "And which arm to play is determined by the equation:\n",
    "\n",
    "![argmax_epsilon](img/epsilon_argmax.png)\n",
    "\n",
    "\n",
    "- So we choose the choose the bandit with the highest upper bound (argmax...)\n",
    "- we play that arm\n",
    "- we update _mu_j\n",
    "- we increment N and Nj\n",
    "\n",
    "#### How is this different to Epsilon-Greedy?\n",
    "\n",
    "- First term: Estimate of the conversion rate (_mu_) -> if an arm gives a higher estimate, exploit it more\n",
    "- Second term: depends on N and Nj\n",
    "    - If N is high (we've played many times), but Nj is low (this arm has been played few times), it means we're not very confident about CTRj, and we explore this more\n",
    "- Finally, as N -> infinity, ln(N)/N approaches 0, so we will just be only using for the true highest conversion rate in the limit (without having to \"try out B once in a while\" like in the Epsilon-greedy case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Paradigm\n",
    "\n",
    "**Frequentist**\n",
    "- we measure things like mean and CTR with point estimates\n",
    "    - e.g. sum(X)/N\n",
    "- to measure how accurate these point estimates are , we use confidence intervals and the Central Limit Theorem\n",
    "- as well as teh Maximum likelihood solution\n",
    "\n",
    "theta_hat = _argmax__theta*P(X|theta)\n",
    "\n",
    "**Bayesian**\n",
    "- treat theta as a random variable too, so it has its own distribution\n",
    "\n",
    "![bayesian_prob](img/bayesian_estimate_prob.png)\n",
    "\n",
    "- Z = normalizing constant\n",
    "- P(X|theta) = likelihood (how likely is this data given current theta?)\n",
    "- P(theta) = prior (old belief about theta)\n",
    "- P(theta|X) = posterior (new belief about theta after seeing the data)\n",
    "\n",
    "\n",
    "**how does it actually work?**\n",
    "- these are all probability distributions!\n",
    "- P(X) is the integral over P(X|theta)P(theta)dtheta\n",
    "    - generally speaking, this is either hard or impossible to solve\n",
    "    - One solution is to use sampling methods like Markov Chains Monte Carlo\n",
    "    \n",
    "If we **don't** want to use something like MCMC, we can use another, more elegant solution, called **Conjugate Priors**\n",
    "\n",
    "### Conjugate Priors\n",
    "- they give an elegant solution for P(theta|X)\n",
    "- If we choose specific distributions fo P(X|theta) and P(theta), then we sort of know (\"make\") P(theta|X) the same type of distribution as P(theta)\n",
    "- \"for specific likelihood distributions and for specific prior distributions, the posterior distribution will be the same as the prior distribution\"\n",
    "\n",
    "**Example**\n",
    "1. We know that the likelihood for click-through rate is Bernoulli: \n",
    "\n",
    "![bernoulli_likelihood](img/bernoulli_likelihood.png)\n",
    "\n",
    "2. We also know that theta here must be between 0 and 1, because it's the probability of a click. \n",
    "    - This can be modelled as the Beta distribution \n",
    "    \n",
    "![beta-dist](img/beta_dist.png)\n",
    "\n",
    "How do we solve for this?\n",
    "\n",
    "![conj_prior](img/conjugate_prior_ctr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alpha and beta get bigger, the variance gets smaller so we are more confident.\n",
    "\n",
    "How do we choose the prior alpha and beta?\n",
    "- We can choose any, such as alpha = 1 and beta = 1 (uniform)\n",
    "\n",
    "#### Choosing conjugate priors\n",
    "How do we know which prior pairs are conjugate?\n",
    "[Wikipedia article](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian A/B testing in practice\n",
    "\n",
    "1. How can we use the conjugate priors to solve the explore-exploit dilemma?\n",
    "2. How can we do it better than the epsilon-greedy and UCB1 algorithms?\n",
    "\n",
    "#### Click-through rate example\n",
    "**Sampling**\n",
    "`scipy` allows us to sample from any distribution we want\n",
    "- We'll sample for the beta distribution\n",
    "\n",
    "##### Scenario 1: we've already explored a lot\n",
    "\n",
    "![scenario1](img/scenario1.png)\n",
    "\n",
    "- we have sharp (low variance), high confidence estimates of our click through rates\n",
    "- if we sample from these 2 beta dist, we expect with high probability that the one with the higher CTR will give us a higher random number\n",
    "    - i.e. choose the bandit that gives us the largest random number from these two beta distributions. \n",
    "    - it is still possible for the worse bandit to give us a higher random number\n",
    "    \n",
    "##### Scenario 2: one bandit has been exploited a lot\n",
    "\n",
    "![scenario2](img/scenario2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
